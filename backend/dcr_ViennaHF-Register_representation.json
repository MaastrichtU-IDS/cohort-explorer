{"dataScienceDataRoom": {"v10": {"interactive": {"commits": [], "enableAutomergeFeature": true, "initialConfiguration": {"description": "A data clean room to provision the data for the ViennaHF-Register cohort", "enableAirlock": true, "enableAllowEmptyFilesInValidation": true, "enableDevelopment": true, "enablePostWorker": true, "enableSafePythonWorkerStacktrace": true, "enableServersideWasmValidation": true, "enableSqliteWorker": true, "enableTestDatasets": true, "enclaveRootCertificatePem": "-----BEGIN CERTIFICATE-----\nMIIDLjCCAhagAwIBAgIUJrCo0qaUomVXfpv8MuMQATh1Ta0wDQYJKoZIhvcNAQEL\nBQAwHTEbMBkGA1UEAxMSRGVjZW50cmlxIFBLSSBSb290MB4XDTIxMDYwOTA3MDIy\nM1oXDTMxMDYwNzA3MDI1M1owHTEbMBkGA1UEAxMSRGVjZW50cmlxIFBLSSBSb290\nMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA5RHa2DYFKbAd1rbQ78mt\nwOfqqpWs/9z8DyoIZNi9k3e3VwCaMw7FIhioeAL1I3g+B+B9rWK8XWH3maUB4tLJ\nUc9NXt1/24ameUdZiPbQn3WXJoX6cOuc8+uvYtRm6dRqBkp9++rYSXB3llSFbzj7\nMXfrFg/lFLnmEQsrJbAbhmgrQL2oQLxnn14rJUd1ny5zE3bJjikHxxBGmV8JXNxZ\nm/Z0cfSz+s88y47TkYzTZRDc6hszTab78RULTGLidNmFsf+wrtqZzC8RXH55F4g6\nn1HZcZTqn6j7Ndksi1k3UtDoFnzxMeH1/zfSL954hK9JiQloKG9MbtpxakCQ2z8m\nswIDAQABo2YwZDAOBgNVHQ8BAf8EBAMCAQYwEgYDVR0TAQH/BAgwBgEB/wIBADAd\nBgNVHQ4EFgQUWmLfdghaz+kdENZh8Ozx+7ej6VAwHwYDVR0jBBgwFoAUWmLfdgha\nz+kdENZh8Ozx+7ej6VAwDQYJKoZIhvcNAQELBQADggEBAFIILXUAHDqTmd4qR8KQ\nv1ywfYEZNvSDcW8S2upJd5uxy1JmdcXhmvR5LWMRDeqZbxK/tOkFJCcxI14xpVX4\n+0OxkVOuxXt9CTPw/Y2RUjaZd0gRtxxBd7TTaOj8l6RrPIR4ynEddTOalO8r1Ne4\nZqxEqsOnAbgTAFX9QhFBr6+cpPOO8KLM6aQ4ShcoGFuZTl812TXJr42/Mh9+76nf\n2JjqTY/vDjzbfHOctYxv8i6w6qrC3GCXGuJa2lOWnYFJbGLVgTAvhjzdsh7wy52B\nLp0IOcEedDEr9LjTmTIU6hMxX4/Jh9eFVLFVy9aFZYX1/Rv9UVNLDH7GCe5lYTqk\npzs=\n-----END CERTIFICATE-----", "enclaveSpecifications": [{"attestationProtoBase64": "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", "id": "decentriq.python-ml-worker-32-64", "workerProtocol": 1}, {"attestationProtoBase64": "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", "id": "decentriq.driver", "workerProtocol": 1}], "enableForceSparkValidation": false, "id": "595d9233-3e52-44cf-a376-e2ce368e1cb3", "nodes": [{"id": "ViennaHF-Register", "name": "ViennaHF-Register", "kind": {"leaf": {"isRequired": true, "kind": {"raw": {}}}}}, {"id": "ViennaHF-Register-metadata", "name": "ViennaHF-Register-metadata", "kind": {"leaf": {"isRequired": true, "kind": {"table": {"columns": [{"name": "VARIABLENAME", "dataFormat": {"isNullable": true, "dataType": "string"}, "validation": {"name": "VARIABLENAME", "formatType": "STRING", "allowNull": true}}, {"name": "VARIABLELABEL", "dataFormat": {"isNullable": true, "dataType": "string"}, "validation": {"name": "VARIABLELABEL", "formatType": "STRING", "allowNull": true}}, {"name": "VARTYPE", "dataFormat": {"isNullable": true, "dataType": "string"}, "validation": {"name": "VARTYPE", "formatType": "STRING", "allowNull": true}}, {"name": "UNITS", "dataFormat": {"isNullable": true, "dataType": "string"}, "validation": {"name": "UNITS", "formatType": "STRING", "allowNull": true}}, {"name": "CATEGORICAL", "dataFormat": {"isNullable": true, "dataType": "string"}, "validation": {"name": "CATEGORICAL", "formatType": "STRING", "allowNull": true}}, {"name": "MISSING", "dataFormat": {"isNullable": true, "dataType": "string"}, "validation": {"name": "MISSING", "formatType": "STRING", "allowNull": true}}, {"name": "COUNT", "dataFormat": {"isNullable": true, "dataType": "integer"}, "validation": {"name": "COUNT", "formatType": "INTEGER", "allowNull": true}}, {"name": "NA", "dataFormat": {"isNullable": true, "dataType": "integer"}, "validation": {"name": "NA", "formatType": "INTEGER", "allowNull": true}}, {"name": "MIN", "dataFormat": {"isNullable": true, "dataType": "string"}, "validation": {"name": "MIN", "formatType": "STRING", "allowNull": true}}, {"name": "MAX", "dataFormat": {"isNullable": true, "dataType": "string"}, "validation": {"name": "MAX", "formatType": "STRING", "allowNull": true}}, {"name": "Formula", "dataFormat": {"isNullable": true, "dataType": "string"}, "validation": {"name": "Formula", "formatType": "STRING", "allowNull": true}}, {"name": "Categorical Value Concept Code", "dataFormat": {"isNullable": true, "dataType": "string"}, "validation": {"name": "Categorical Value Concept Code", "formatType": "STRING", "allowNull": true}}, {"name": "Categorical Value Concept Name", "dataFormat": {"isNullable": true, "dataType": "string"}, "validation": {"name": "Categorical Value Concept Name", "formatType": "STRING", "allowNull": true}}, {"name": "Categorical Value OMOP ID", "dataFormat": {"isNullable": true, "dataType": "string"}, "validation": {"name": "Categorical Value OMOP ID", "formatType": "STRING", "allowNull": true}}, {"name": "Variable Concept Code", "dataFormat": {"isNullable": true, "dataType": "string"}, "validation": {"name": "Variable Concept Code", "formatType": "STRING", "allowNull": true}}, {"name": "Variable Concept Name", "dataFormat": {"isNullable": true, "dataType": "string"}, "validation": {"name": "Variable Concept Name", "formatType": "STRING", "allowNull": true}}, {"name": "Variable OMOP ID", "dataFormat": {"isNullable": true, "dataType": "string"}, "validation": {"name": "Variable OMOP ID", "formatType": "STRING", "allowNull": true}}, {"name": "Additional Context Concept Name", "dataFormat": {"isNullable": true, "dataType": "string"}, "validation": {"name": "Additional Context Concept Name", "formatType": "STRING", "allowNull": true}}, {"name": "Additional Context Concept Code", "dataFormat": {"isNullable": true, "dataType": "string"}, "validation": {"name": "Additional Context Concept Code", "formatType": "STRING", "allowNull": true}}, {"name": "Additional Context OMOP ID", "dataFormat": {"isNullable": true, "dataType": "string"}, "validation": {"name": "Additional Context OMOP ID", "formatType": "STRING", "allowNull": true}}, {"name": "Unit Concept Name", "dataFormat": {"isNullable": true, "dataType": "string"}, "validation": {"name": "Unit Concept Name", "formatType": "STRING", "allowNull": true}}, {"name": "Unit Concept Code", "dataFormat": {"isNullable": true, "dataType": "string"}, "validation": {"name": "Unit Concept Code", "formatType": "STRING", "allowNull": true}}, {"name": "Unit OMOP ID", "dataFormat": {"isNullable": true, "dataType": "integer"}, "validation": {"name": "Unit OMOP ID", "formatType": "INTEGER", "allowNull": true}}, {"name": "Domain", "dataFormat": {"isNullable": true, "dataType": "string"}, "validation": {"name": "Domain", "formatType": "STRING", "allowNull": true}}, {"name": "Visits", "dataFormat": {"isNullable": true, "dataType": "string"}, "validation": {"name": "Visits", "formatType": "STRING", "allowNull": true}}, {"name": "Visit OMOP ID", "dataFormat": {"isNullable": true, "dataType": "integer"}, "validation": {"name": "Visit OMOP ID", "formatType": "INTEGER", "allowNull": true}}, {"name": "Visit Concept Name", "dataFormat": {"isNullable": true, "dataType": "string"}, "validation": {"name": "Visit Concept Name", "formatType": "STRING", "allowNull": true}}, {"name": "Visit Concept Code", "dataFormat": {"isNullable": true, "dataType": "string"}, "validation": {"name": "Visit Concept Code", "formatType": "STRING", "allowNull": true}}], "validationNode": {"staticContentSpecificationId": "decentriq.driver", "pythonSpecificationId": "decentriq.python-ml-worker-32-64", "validation": {}, "dropInvalidRows": false}}}}}}, {"id": "c1_data_dict_check", "name": "c1_data_dict_check", "kind": {"computation": {"kind": {"scripting": {"additionalScripts": [], "dependencies": ["ViennaHF-Register-metadata", "ViennaHF-Register"], "enableLogsOnError": false, "enableLogsOnSuccess": false, "mainScript": {"name": "python-script", "content": "\nimport pandas as pd\nimport decentriq_util\n\n# Load the metadata dictionary\ndictionary_df = decentriq_util.read_tabular_data(\"/input/ViennaHF-Register-metadata\")\ntry:\n    varname_col = [x for x in ['VARIABLE NAME', 'VARIABLENAME', 'VAR NAME'] if x in dictionary_df.columns][0]\nexcept:\n    raise ValueError(\"The dictionary file does not contain a 'VARIABLE NAME'/'VARIABLENAME' column.\")\n\nprint(\"metadata variable names: \", [v for v in dictionary_df[varname_col]])\n\n# Load the dataset\ntry:\n    dataset_df = pd.read_csv(\"/input/ViennaHF-Register\")\n    #dataset_df = decentriq_util.read_tabular_data(\"/input/ViennaHF-Register\")\nexcept Exception as e:\n    try:\n        dataset_df = pd.read_spss(\"/input/ViennaHF-Register\")\n    except Exception as e2:\n        raise ValueError(\"The dataset file does appear to be a valid CSV or SPSS file.CSV error: \" + str(e) + \"SPSS error: \" + str(e2))\n    \n    \n\n# Extract 'VARIABLE NAME' column from dictionary and dataset column names\ndictionary_variables = set([x.strip() for x in dictionary_df[varname_col].unique()])\ndataset_columns = set([x.strip() for x in dataset_df.columns])\n\n# Compare the sets\nin_dictionary_not_in_dataset = dictionary_variables - dataset_columns\nin_dataset_not_in_dictionary = dataset_columns - dictionary_variables\n\n# Optionally save the results to files for reference\npd.DataFrame({'In Dataset Not in Dictionary': list(in_dataset_not_in_dictionary)}).to_csv(\"/output/in_dataset_not_in_dictionary.csv\",index = False)\npd.DataFrame({'In Dictionary Not in Dataset': list(in_dictionary_not_in_dataset)}).to_csv(\"/output/in_dictionary_not_in_dataset.csv\",index = False)\n#print(\"variable names: \", [v for v in dictionary_df[varname_col]])\n"}, "output": "/output", "scriptingLanguage": {"python": {}}, "scriptingSpecificationId": "decentriq.python-ml-worker-32-64", "staticContentSpecificationId": "decentriq.driver"}}}}}, {"id": "c2_save_to_json", "name": "c2_save_to_json", "kind": {"computation": {"kind": {"scripting": {"additionalScripts": [], "dependencies": ["ViennaHF-Register-metadata", "ViennaHF-Register"], "enableLogsOnError": false, "enableLogsOnSuccess": false, "mainScript": {"name": "python-script", "content": "import decentriq_util\nimport pandas as pd\nimport os\nimport json\nfrom pprint import pprint\n\n\ndef _column_is_date(series):\n    try:\n        pd.to_datetime(series)\n        return True\n    except:\n        return False\n\ndef _column_is_float(series):\n    try:\n        non_na = series.dropna()\n        float_series = non_na.astype(float)\n        return True\n    except:\n        return False\n\ndef _column_is_numeric(series):\n    #meaning integers\n    try:\n        if series.dropna().apply(lambda x: str(x).isdigit() or str(x).endswith('.0')).all():\n            return True\n        else:\n            return False\n    except:\n        return False\n        \ndef _cast_col(series, typ):\n    if typ == 'date':\n        ns = pd.to_datetime(series, errors='coerce').dt.date\n    elif typ == 'int':\n        ns = pd.to_numeric(series, errors='coerce').astype('Int64')\n    elif typ == 'float':\n        ns = pd.to_numeric(series, errors='coerce')\n    else:\n        print(\"unrecognized type\")\n        return series, []\n    invalid_cells = []\n    for i, (c1, c2) in enumerate(zip(series, ns)):\n        if pd.notna(c1) and pd.isna(c2):\n            invalid_cells.append(i)\n    return ns, invalid_cells\n\n# Load dictionary\ndictionary = decentriq_util.read_tabular_data(\"/input/ViennaHF-Register-metadata\")\n\n# Clean column names to ensure uniformity\ndictionary.columns = dictionary.columns.str.strip().str.upper()\n\nvarname_col = [x for x in ['VARIABLE NAME', 'VARIABLENAME', 'VAR NAME'] if x in dictionary.columns][0]\nvartype_col = [x for x in ['VAR TYPE', 'VARTYPE'] if x in dictionary.columns][0]\nvarlabel_col = [x for x in ['VARIABLE LABEL', 'VARIABLELABEL', 'VAR LABEL'] if x in dictionary.columns][0]\n\ndictionary[varname_col] = dictionary[varname_col].str.strip().str.lower()\ndictionary[vartype_col] = dictionary[vartype_col].str.strip().str.lower()\n\ntry:\n    data = pd.read_csv(\"/input/ViennaHF-Register\", na_values=[''], keep_default_na=False)\n    #data = decentriq_util.read_tabular_data(\"/input/ViennaHF-Register\")\nexcept Exception as e:\n    data = pd.read_spss(\"/input/ViennaHF-Register\")\n\n\n#Convert whitespace-only strings to NaN\nfor col in data.select_dtypes(include=['object']):\n    data[col] = data[col].apply(lambda x: pd.NA if isinstance(x, str) and x.isspace() else x)\n\ndata.columns = [c.lower().strip() for c in data.columns]\n\n#for col in data.columns:\n#    try:\n#        if data[col].dropna().apply(lambda x: str(x).isdigit() or str(x).endswith('.0')).all():\n#            data[col] = data[col].astype('Int64')\n#    except:\n#        continue\n\n# Define the pattern for entries to exclude non-categorical variables\n#include_pattern = r'\\||='   # Look for strings containing either a | or =.\n# Exclude rows in the dictionary where the 'CATEGORICAL' column contains the defined pattern\n# categorical_dict = dictionary[dictionary['CATEGORICAL'].astype(str).str.contains(include_pattern, regex=True)]\n\nvars_to_process = {}\n# Prepare to extract classes and their meanings, along with MIN, MAX, and VAR TYPE\nvars_details = {}\nmismatched_types = {}\n\nfor index, row in dictionary.iterrows():\n    variable_name = row[varname_col]\n    var_type = row[vartype_col]\n    categories_info = row['CATEGORICAL']\n\n    if variable_name.lower() in ['patientid', 'pat.id', 'pati\u00ebntnummer']:\n        continue\n        \n    if variable_name.lower() not in data.columns:\n        continue\n\n    if (pd.notna(categories_info) and isinstance(categories_info, str) and categories_info.strip() != \"\"):\n        vars_to_process[variable_name] = 'categorical'\n    elif _column_is_numeric(data[variable_name]):\n        if data[variable_name].nunique()>9:\n            vars_to_process[variable_name] = 'int'\n        else:\n            vars_to_process[variable_name] = 'categorical'\n    elif _column_is_float(data[variable_name]):\n        vars_to_process[variable_name] = 'float'\n    elif _column_is_date(data[variable_name]):\n        vars_to_process[variable_name] = 'date'\n    elif data[variable_name].nunique()>20:\n        #assume float:\n        vars_to_process[variable_name] = 'float'\n    else: #fewer than 20 unique\n        #assume categorical:\n        print(\"The following variable deemed categorical by process of elimination: \", variable_name)\n        vars_to_process[variable_name] = 'categorical'\n\n    if ((var_type.lower() == \"datetime\" and vars_to_process[variable_name] != \"date\") or \n        var_type.lower() == \"str\" and vars_to_process[variable_name] != \"categorical\"):\n        mismatched_types[variable_name] = {\"declared\": var_type, \"inferred\": vars_to_process[variable_name]}\n\n#find the mismatches between declared types (in data dictionary) and inferred types:\n\nfor index, row in dictionary.iterrows():\n    variable_name = row[varname_col]\n    var_type = row[vartype_col]\n    if variable_name not in vars_to_process:\n        continue\n    else:\n        t = vars_to_process[variable_name]\n    categories_info = row['CATEGORICAL']\n\n    if t == 'categorical':\n        #categories = [item for sublist in categories_info.split('|') for item in sublist.split(',')]\n        categories = [item.strip() for item in categories_info.split('|')]\n        class_names = {}\n\n        for category in categories:\n                key_value = category.lower().split('=')\n                if len(key_value) == 2:\n                    #print(\"inside if statement: \", variable_name, key_value)\n                    key = key_value[0].strip()\n                    value = key_value[1].upper().strip()\n                    class_names[key] = value\n                elif len(key_value) == 1:  \n                    #category does not have \"=\"\n                    class_names[key_value[0].strip().upper()] = key_value[0].strip().upper()\n                else:\n                    msg = f\"Encountered a possible parsing error. Check category info for variable {variable_name}, {key_value}, Full category info: {categories_info}\"\n                    mismatched_types[variable_name + \"-categories\"] =  msg\n                    print(msg)\n\n        # Check if there is a value that corresponds to  'missing'\n        if 'MISSING' in class_names.values():\n            missing_key = [x[0] for x in class_names.items() if x[1] == 'MISSING'][0]\n            print(\"MISSING value exists among categories: \", variable_name, missing_key)\n        elif 'MISSING' in dictionary.columns and row['MISSING'].strip() != \"\":\n            missing_key = str(row['MISSING']).strip()\n            print(f\"MISSING value {missing_key} declared for variable: \", variable_name)\n        else:\n            print(\"No 'missing' value for variable \", variable_name)\n            #needed the line below, otherwise the \"missing_key\" will still store the value for the previous var\n            missing_key = None\n\n    else: #ints or floats or dates:\n        missing_key = str(row['MISSING']).strip() if 'MISSING' in dictionary.columns and row['MISSING'].strip() != \"\" else None\n\n    if missing_key == None:\n        count_missing = 0\n    else:\n        count_missing = (data[variable_name].astype(str).str.strip().str.upper() == str(missing_key).strip().upper()).sum()\n\n    na_count = data[variable_name].isna().sum()\n\n    vars_details[variable_name] = {\n            'var_label': row[varlabel_col],\n            'missing': missing_key,\n            'declared_type': var_type,\n            'inferred_type': t,\n            'count_missing': int(count_missing),\n            'count_na': int(na_count)\n    }\n    if t == 'categorical':\n        vars_details[variable_name]['categories'] = class_names\n\njson_dir = '/output/'\n\n# Save all variable details to a JSON file\nvars_details_json_path = os.path.join(json_dir, 'variable_details.json')\nwith open(vars_details_json_path, 'w') as json_file:\n    json.dump(vars_details, json_file, indent=4)\n\n# Print confirmation messages and the first 5 items in a formatted way\nprint(f\"Variable details saved to {vars_details_json_path}\")\nprint(json.dumps({key: vars_details[key] for key in sorted(list(vars_details.keys()))[:-1]}, indent=4))\n            \nall_data_issues = [str(i) for i in mismatched_types.items()]\n\ndata_issues_json_path = os.path.join(json_dir, 'data_issues.json')\nwith open(data_issues_json_path, 'w') as json_file:\n    json.dump(all_data_issues, json_file, indent=4)\npprint(all_data_issues)\n"}, "output": "/output", "scriptingLanguage": {"python": {}}, "scriptingSpecificationId": "decentriq.python-ml-worker-32-64", "staticContentSpecificationId": "decentriq.driver"}}}}}, {"id": "c3_eda_data_profiling", "name": "c3_eda_data_profiling", "kind": {"computation": {"kind": {"scripting": {"additionalScripts": [], "dependencies": ["c1_data_dict_check", "c2_save_to_json", "ViennaHF-Register-metadata", "ViennaHF-Register"], "enableLogsOnError": false, "enableLogsOnSuccess": false, "mainScript": {"name": "python-script", "content": "\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport seaborn as sns\nfrom scipy.stats import shapiro, skew, kurtosis, zscore\nimport warnings\nimport decentriq_util\nimport re\nfrom datetime import datetime\nimport json\nimport collections.abc\nfrom collections import OrderedDict\nwarnings.filterwarnings('ignore')\n\n\n\n# Load the dataset with corrected missing values replaced with NA from previous step\n# data_correct_missing = pd.read_csv(\"/input/C3_map_missing_do_not_run/data_correct.csv\", low_memory=False)\n\n#Load the JSON files from C2\nvars_details = pd.read_json(\"/input/c2_save_to_json/variable_details.json\")\n\nwith open(\"/input/c2_save_to_json/data_issues.json\") as f:\n    data_issues = json.load(f)\n\ntry:\n    data = pd.read_csv(\"/input/ViennaHF-Register\", na_values=[''], keep_default_na=False)\n    #data = decentriq_util.read_tabular_data(\"/input/ViennaHF-Register\")\nexcept Exception as e:\n    data = pd.read_spss(\"/input/ViennaHF-Register\")\n\nfor col in data.select_dtypes(include=['object']):\n    data[col] = data[col].apply(lambda x: pd.NA if isinstance(x, str) and x.isspace() else x)\n\ndata.columns = [c.lower().strip() for c in data.columns]\n\n\ndef _cast_col(series, typ):\n    if typ == 'date':\n        ns = pd.to_datetime(series, errors='coerce').dt.date\n    elif typ == 'int':\n        ns = pd.to_numeric(series, errors='coerce').astype('Int64')\n    elif typ == 'float':\n        ns = pd.to_numeric(series, errors='coerce')\n    elif typ == 'categorical':\n        try:\n            ns = series.astype('Int64').astype(str)\n        except:\n            ns = series.astype(str)\n    else:\n        print(\"unrecognized type\")\n        return series, []\n    invalid_cells = []\n    for i, (c1, c2) in enumerate(zip(series, ns)):\n        if pd.notna(c1) and pd.isna(c2):\n            invalid_cells.append(i)\n    return ns, invalid_cells\n\n\n#type casting the data:\nfor v, d in vars_details.items():\n    try:\n        cast_col, inv_cells = _cast_col(data[v], d['inferred_type'])\n    except Exception as e:\n        msg = f\"Error while attempting to cast {v} to {d['inferred_type']}: {e}\"\n        print(msg)\n        data_issues.append(msg)\n        continue\n    data[v] = cast_col\n    if len(inv_cells)>0:\n        data_issues.append(f\"Column {v} the following cells appears to be invalid: {str(inv_cells)}\")\n\n\n\n#variables that should be graphed\n#vars_to_graph = ['age', 'weight', 'cough1', 'angina1', 'hscrp_v6']\n#vars_to_graph = ['age', 'ALCOOL', 'ALLOPURI', 'ALT', 'ALTACE', 'ALTANO', 'COLETOT', 'CREATIN', 'DALTACE', 'DATAECG', 'DATALAB']\n#vars_to_graph = ['age', 'ALCOOL', 'DATAECG', 'DATALAB']\n#vars_to_graph = [x.lower() for x in vars_to_graph]\n#vars_to_graph = ['age', 'ALCOOL', 'DATAECG', 'DATALAB', 'AATHORAX', 'AATHORAXDIM', 'ACE_AT_V1']\nvars_to_graph = list(vars_details.columns)\nvars_to_graph = [x.strip().lower() for x in vars_to_graph]\n\ndef _lowercase_if_string(x):\n    if isinstance(x, str):\n        return x.lower()\n    return x\n\ndef variable_eda(df, vars_details):\n    vars_stats = {}\n    graph_tick_data = {}\n    df.columns = df.columns.str.lower().str.strip()\n    for column in df.columns.tolist():\n        if column not in vars_details.columns:\n            continue\n        # Continuous variables\n        try:\n            if 'missing' in vars_details[column] and vars_details[column]['missing']:\n                if vars_details[column]['inferred_type'] in ['int']:\n                    df[column].replace(int(vars_details[column]['missing']), pd.NA, inplace=True)\n                elif vars_details[column]['inferred_type'] in ['float']:\n                    df[column].replace(float(vars_details[column]['missing']), pd.NA, inplace=True)\n                elif vars_details[column]['inferred_type'] in ['date']:\n                    df[column].replace(vars_details[column]['missing'], pd.NA, inplace=True)\n                else: #categorical\n                    df[column].replace(vars_details[column]['missing'], '<missing>', inplace=True)\n        \n            if vars_details[column]['inferred_type'] in ['int', 'float']:\n\n                #if not pd.api.types.is_numeric_dtype(df[column]):\n                    # Skip if the column is not numeric\n                    # print(\"Column \", column, \" skipped because non-numeric\")\n                    #continue\n\n                # Descriptive Stats\n                stats = df[column].describe()\n                mode_value = df[column].mode()[0] if not df[column].mode().empty else np.nan\n                value_counts = df[column].value_counts(dropna=False)\n                count_missing = vars_details[column]['count_missing']\n                missing_percent = count_missing / len(df) * 100\n                count_na = vars_details[column]['count_na']\n                #try:\n                #    empty = df[column].isnull().sum() + df[column].str.strip().eq('').sum()\n                #except:\n                #    empty = df[column].isnull().sum()\n\n                # Check for numeric values before computing skewness and kurtosis\n                if len(df[column].dropna()) > 0:\n                    #print(f\"Debug {column}: dtype={df[column].dtype}, first 5 values={df[column].head()}\")\n                    #print(f\"After dropna: dtype={df[column].dropna().dtype}, count={len(df[column].dropna())}\")\n                    #df[column] = pd.to_numeric(df[column], errors='coerce')\n                    #column_no_na = df[column].dropna()\n                    #print(f\"After numeric coerce and dropping na: dtype={column_no_na.dtype}, count={column_no_na}\")\n                    #print(\"type of column no na: \", type(column_no_na))\n                    #print(\"type of column_no_na values: \", type(column_no_na.values))\n                    #df[column] = df[column].astype(float)\n                    skewness = skew([_ for _ in df[column].dropna()], bias=False)\n                    #skewness = df[column].skew()\n                    kurt = kurtosis([_ for _ in df[column].dropna()], bias=False)\n                else:\n                    skewness = np.nan\n                    kurt = np.nan\n\n                # Normality Test\n                if len(df[column].dropna()) > 3:  # Shapiro requires at least 3 values\n                    w_test_stat, p = shapiro(df[column].dropna())\n                    normality = \"Normal\" if p > 0.05 else \"Non-Normal\"\n                    p_value_str = f\"{p:.4f}\"\n                else:\n                    normality = \"Insufficient Data\"\n                    p_value_str = \"N/A\"\n\n                # Outlier Detection (IQR Method)\n                Q1 = stats['25%']\n                Q3 = stats['75%']\n                IQR = Q3 - Q1\n                lower_bound = Q1 - 1.5 * IQR\n                upper_bound = Q3 + 1.5 * IQR\n                outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)][column].count()\n\n                # Z-Scores for Outliers\n                z_scores = zscore([_ for _ in df[column].dropna()]) if len(df[column].dropna()) > 0 else np.array([])\n                z_outliers = (np.abs(z_scores) > 3).sum() if z_scores.size > 0 else 0\n\n                # Range Calculation\n                range_value = stats['max'] - stats['min']\n                count_nonnull = int(stats['count'])-int(count_missing)\n\n                # Stats Text\n                stats_text = (\n                    f\"Column: {column}\",\n                    f\"Label: {vars_details[column]['var_label']}\",\n                    f\"Type: Numeric (encoded as {df[column].dtype})\",\n                    f\"Count of observations (ex. missing/empty): {count_nonnull}\",\n                    f\"Count empty:         {count_na} ({(count_na/len(df[column])) * 100:.2f}%)\",\n                    f\"Count missing:       {count_missing} ({(count_missing/len(df[column])) * 100:.2f}%)\",\n                    f\"Code for missing value: {vars_details[column]['missing']}\",\n                    f\"Number of Unique Values/Categories: {df[column].nunique()}\",\n                    f\"Mean:                {stats['mean']:.2f}\",\n                    f\"Median:              {stats['50%']:.2f}\",\n                    f\"Mode:                {mode_value:.2f}\",\n                    f\"Std Dev:             {stats['std']:.2f}\",\n                    f\"Variance:            {stats['std']**2:.2f}\",\n                    f\"Max:                 {stats['max']:.2f}\",\n                    f\"Min:                 {stats['min']:.2f}\",\n                    f\"Range:               {range_value:.2f}\", \n                    f\"Q1:                  {Q1:.2f}\",\n                    f\"Q3:                  {Q3:.2f}\",\n                    f\"IQR:                 {IQR:.2f}\",\n                    f\"Outliers (IQR):      {outliers} ({(outliers / len(df)) * 100:.2f}%)\",\n                    f\"Outliers (Z):        {z_outliers}\",\n                    f\"Skewness:            {skewness:.2f}\",\n                    f\"Kurtosis:            {kurt:.2f}\",\n                    f\"W_Test:              {w_test_stat:.2f}\",\n                    f\"Normality Test: p-value={p_value_str} => {normality}\"\n                )\n\n                if column in vars_to_graph:\n                    try:\n                        graph_tick_data[column] = create_save_graph(df, column, stats_text, 'numerical')\n                    except Exception as e:\n                        data_issues.append(f\"Failed to create a graph for column {column}. Exception msg: {str(e)}\")\n\n\n            # Categorical variables\n            elif vars_details[column]['inferred_type'] == 'categorical':\n                stats = df[column].describe()\n                value_counts = df[column].apply(_lowercase_if_string).value_counts(dropna=False)\n                total = len(df)\n                \n\n                # Get the categories mapping and normalize keys\n                categories_mapping = vars_details[column].get(\"categories\", [])\n                categories_mapping = {str(k): v for (k, v) in categories_mapping.items()}\n                #print(\"variable: \", column, \"categories:\", categories_mapping, value_counts )\n\n                if value_counts.empty:\n                    stats_text = (\n                        f\"Column: {column}\",\n                        f\"Label: {vars_details[column]['var_label']}\",\n                        f\"Type: Categorical (encoded as {df[column].dtype})\",\n                        f\"Number of Unique Categories: 0\",\n                        f\"Missing Values: {df[column].isnull().sum()} ({df[column].isnull().mean() * 100:.2f}%)\"\n                    )\n                else:\n                    # Chi-square test\n                    expected = total / len(value_counts)\n                    chi_square_stat = ((value_counts - expected) ** 2 / expected).sum()\n                    count_missing = vars_details[column][\"count_missing\"]\n                    count_na = vars_details[column][\"count_na\"]\n                    count_nonnull = int(stats['count'])-int(count_missing)\n\n                    # Class balance with corrected mapping\n                    class_balance_text = \"\\n\\t\"\n                    for key, count in value_counts.items():\n                        if str(key) in categories_mapping and str(key) != categories_mapping[str(key)]:\n                            class_balance_text += f\"{(key, categories_mapping[str(key)])} -> {round(count / total * 100, 2)}%\\n\t\"\n                        else:\n                            class_balance_text += f\"{key} -> {round(count / total * 100, 2)}%\\n\t\"\n\n                    stats_text = (\n                        f\"Column: {column}\",\n                        f\"Label: {vars_details[column]['var_label']}\",\n                        f\"Type: Categorical (encoded as {df[column].dtype})\",\n                        f\"Number of unique values/categories: {len(value_counts)}\",\n                        f\"Most frequent category: {categories_mapping.get(str(value_counts.idxmax()), 'Unknown')} \",\n                        f\"Count of observations (ex. missing/empty): {count_nonnull}\",\n                        f\"Count empty: {count_na} ({(count_na/len(df[column])) * 100:.2f}%)\",\n                        f\"Count missing: {count_missing} ({(count_missing/len(df[column])) * 100:.2f}%)\",\n                        f\"Code for missing value: {vars_details[column]['missing']}\",\n                        f\"Class balance: {class_balance_text}\",\n                        f\"Chi-Square Test Statistic: {chi_square_stat:.2f}\"\n                    )\n\n                if column in vars_to_graph:\n                    try:\n                        graph_tick_data[column] = create_save_graph(df, column, stats_text, 'categorical', category_mapping = categories_mapping)\n                    except Exception as e:\n                        data_issues.append(f\"Failed to create a graph for column {column}. Exception msg: {str(e)}\")\n                        \n                        \n            \n            elif vars_details[column]['inferred_type'] == 'date':\n                try:\n                    stats = pd.to_datetime(df[column], format='mixed').describe()\n                except:\n                    continue\n                value_counts = df[column].value_counts(dropna=False)\n                total = len(df)\n                count_missing = vars_details[column][\"count_missing\"]\n                count_na = vars_details[column][\"count_na\"]\n                count_nonnull = int(stats['count'])-int(count_missing)\n                stats_text = [\n                        f\"Column: {column}\",\n                        f\"Label: {vars_details[column]['var_label']}\",\n                        f\"Type: Date (encoded as {df[column].dtype})\",\n                        f\"Number of unique values: {len(value_counts)}\",\n                        f\"Most frequent value: {str(value_counts.idxmax()).split('.')[0]}\",\n                        f\"Count of observations (ex. missing/empty): {count_nonnull}\",\n                        f\"Count missing: {count_missing} ({(count_missing/len(df[column])) * 100:.2f}%)\",\n                        f\"Count empty: {count_na} ({(count_na/len(df[column])) * 100:.2f}%)\",\n                        f\"Mean:                {stats['mean'].date()}\",\n                        f\"Median:              {stats['50%'].date()}\",\n                        f\"Max:                 {stats['max'].date()}\",\n                        f\"Min:                 {stats['min'].date()}\",\n                        f\"Range:               {stats['max'] - stats['min']}\", \n                        f\"Q1:                  {stats['25%'].date()}\",\n                        f\"Q3:                  {stats['75%'].date()}\",\n                        f\"IQR:                 {stats['75%'] - stats['25%']}\",\n                ]\n                #stats_text.extend([f\"{k.capitalize()}: {v}\" for k,v in stats.items()])\n                if column in vars_to_graph:\n                    try:\n                        graph_tick_data[column] = create_save_graph(df, column, stats_text, 'datetime')\n                    except Exception as e:\n                        data_issues.append(f\"Failed to create a graph for column {column}. Exception msg: {str(e)}\")\n            else:\n                print(\"ELSE case: variable name \", column, \"inferred type: \", vars_details[column]['inferred_type'])\n                stats_text = []\n            stats_text_dict = OrderedDict()\n            stats_text_dict.update({i.split(\":\")[0].strip():i.split(\":\")[1].strip() for i in stats_text})\n            if 'Class balance' in stats_text_dict:\n                stats_text_dict['Class balance'].replace(\" ->\", \":\")\n            stats_text_dict['url'] = f\"https://explorer.icare4cvd.eu/api/variable-graph/ViennaHF-Register/{column}\"\n            vars_stats[column] = stats_text_dict\n        except Exception as e:\n            data_issues.append(f\"Failed to perform EDA on column {column}. Exception msg: {str(e)}\")\n            \n    for col, ticks  in graph_tick_data.items():\n        vars_stats[col].update(ticks)\n    return vars_stats\n\n\n\ndef create_save_graph(df, varname, stats_text, vartype, category_mapping=None):\n    if vartype == 'numerical':\n        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n        # Left: Display Summary Stats\n\n        props = dict(boxstyle=\"round,pad=0.5\", facecolor=\"whitesmoke\", alpha=0.8, edgecolor=\"lightgray\")\n        text_obj = axes[0].text(0.05, 0.95, '\\n'.join(stats_text), transform=axes[0].transAxes, fontsize=11, va='top', ha='left', \n        family='monospace',  bbox=props, wrap=True, linespacing=1.5)\n        if hasattr(text_obj, \"_get_wrap_line_width\"):\n            text_obj._get_wrap_line_width = lambda: 420\n        #axes[0].text(0.05, 0.9, , fontsize=10, va='top', ha='left', linespacing=1.2, family='monospace', wrap=True)\n        axes[0].axis(\"off\")\n\n        # Right: Plot histogram\n        sns.histplot(df[varname].dropna(), kde=True, ax=axes[1])\n        axes[0].set_title(f\"Summary Stats for {varname.upper()}\", fontsize=12)\n        axes[1].set_title(f\"Distribution of {varname.upper()}\", fontsize=12)\n        axes[1].tick_params(axis='x')\n        axes[1].set_xlabel(\"Value\")\n        axes[1].set_ylabel(\"Count\")\n\n        # Save the figure for the current feature\n        plt.tight_layout()\n        plt.savefig(f\"/output/{varname.lower()}.png\")\n        #print(f\"figure for {varname} saved!! \")\n       #plt.close()\n    elif vartype == 'datetime':\n        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n        props = dict(boxstyle=\"round,pad=0.5\", facecolor=\"whitesmoke\", alpha=0.8, edgecolor=\"lightgray\")\n        text_obj = axes[0].text(0.05, 0.95, '\\n'.join(stats_text), transform=axes[0].transAxes, fontsize=11, va='top', ha='left', \n        family='monospace',  bbox=props, wrap=True, linespacing=1.5)\n        if hasattr(text_obj, \"_get_wrap_line_width\"):\n            text_obj._get_wrap_line_width = lambda: 420\n        #axes[0].text(0.05, 0.9, , fontsize=10, va='top', ha='left', linespacing=1.2, family='monospace', wrap=True)\n        axes[0].axis(\"off\")\n        axes[0].set_title(f\"Summary Stats for {varname.upper()}\", fontsize=12)\n        try:\n            date_vals =  pd.to_datetime(df[varname].dropna(), format='mixed')\n        except:\n            print(\"supposed date column could not be parsed: \", varname)\n            return {}\n    \n        date_nums = mdates.date2num(date_vals)\n    \n        min_date = date_vals.min()\n        max_date = date_vals.max()\n        date_range = max_date - min_date\n    \n        \n        if date_range.days > 365 * 10:  \n            bin_freq = 'YS'  # Yearly start\n            axes[1].xaxis.set_major_locator(mdates.YearLocator(base=2))\n        elif date_range.days > 365 * 5: \n            bin_freq = 'YS'  # Yearly\n            axes[1].xaxis.set_major_locator(mdates.YearLocator())\n        if date_range.days > 365 :\n            bin_freq = 'Q'  # Quarterly\n            axes[1].xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n        elif date_range.days > 90 :\n            bin_freq = 'M'  \n            axes[1].xaxis.set_major_locator(mdates.MonthLocator())  # Monthly\n        elif date_range.days > 30:\n            bin_freq = 'W'  # Weekly bins\n            axes[1].xaxis.set_major_locator(mdates.WeekdayLocator(interval=1))  # Weekly\n        else:\n            bin_freq = 'D'  # Daily\n            axes[1].xaxis.set_major_locator(mdates.DayLocator(interval=2))\n        \n        bins = mdates.date2num(pd.date_range(min_date, max_date, freq=bin_freq))\n        \n        axes[1].hist(date_nums, bins=bins, alpha=0.7)\n        axes[1].set_title(f\"Distribution of {varname.upper()}\", fontsize=12)\n\n        \n        if date_range.days <= 90:\n            axes[1].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d')) \n        elif date_range.days <= 365 * 2:\n            axes[1].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))  # \"2020-01\" format\n        else:\n            axes[1].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n            \n        axes[1].tick_params(axis='x', rotation=90)\n        axes[1].tick_params(axis='x', which='minor', bottom=False)\n        \n        plt.tight_layout()\n        plt.savefig(f\"/output/{varname.lower()}.png\")\n        #print(f\"figure for {varname} saved!! \")\n\n\n    elif vartype == 'categorical':\n\n        if df[varname].isna().sum() > 0:\n            value_counts = df[varname].value_counts(dropna=False)\n        else:\n            value_counts = df[varname].value_counts(dropna=True)\n        total = len(df)\n        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n        \n\n        # Summary stats text\n        props = dict(boxstyle=\"round,pad=0.5\", facecolor=\"whitesmoke\", alpha=0.8, edgecolor=\"lightgray\")\n        text_obj = axes[0].text(0.05, 0.95, '\\n'.join(stats_text), transform=axes[0].transAxes, fontsize=11, va='top', ha='left', \n        family='monospace', bbox=props, wrap=True, linespacing=1.5)\n        if hasattr(text_obj, \"_get_wrap_line_width\"):\n            text_obj._get_wrap_line_width = lambda: 400\n        #axes[0].text(0.1, 0.5, stats_text, fontsize=10, va='center', ha='left', family='monospace', wrap=True)\n        \n        axes[0].axis(\"off\")\n\n        # Bar chart\n        if not value_counts.empty:\n            colors = sns.color_palette(\"husl\", len(value_counts))\n            ax = value_counts.plot(kind='bar', color=colors, edgecolor='black', ax=axes[1])\n            axes[0].set_title(f\"Summary Stats for {varname.upper()}\", fontsize=12)\n            axes[1].set_title(f\"Distribution of {varname.upper()}\", fontsize=12)\n            ax.set_xlabel(\"Categories\")\n            ax.set_ylabel(\"Count\")\n\n            # Add labels to the bars\n            if len(value_counts)>4:\n                rot = 90\n            else:\n                rot = 0\n            for idx, value in enumerate(value_counts):\n                percentage = (value / total) * 100\n                ax.text(idx, value + total * 0.02, f\"{value}({percentage:.1f}%)\",\n                        ha='center', fontsize=10, rotation = rot)\n\n            # Adjust x-axis labels to be horizontal\n            xticks = []\n            for v in value_counts.index.astype(str):\n                if v in category_mapping:\n                    xticks.append(category_mapping[v])\n                else:\n                    xticks.append(v)\n\n            if len(xticks)>4:\n                ax.set_xticklabels(xticks, rotation=90, fontsize=10)\n            else:\n                ax.set_xticklabels(xticks, rotation=0, fontsize=10)\n\n        \n        plt.ylim(0, max(value_counts.values) * 1.4)\n        plt.tight_layout()\n        plt.savefig(f\"/output/{varname.lower()}.png\")\n\n    #x_ticks = [_.get_text() for _ in axes[1].get_xticklabels()]\n    #x_tick_labels = axes[1].get_xticklables()\n    #y_ticks =  [_.get_text() for _ in axes[1].get_yticklabels()]\n    #y_tick_labels = axes[1].get_yticklabels()\n    x_ticks = axes[1].get_xticklabels()\n    y_ticks =  axes[1].get_yticklabels()\n    return {\"x-ticks\": \" - \".join([str(_) for _ in x_ticks]),\n    # \"x-labels\": \" - \".join([str(_) for _ in x_tick_labels]),\n            \"y-ticks\": \" - \".join([str(_) for _ in y_ticks]), \n        #\"y-labels\": \" - \".join([str(_) for _ in y_tick_labels])\n        }\n\n\ndef integrate_eda_with_metadata(vars_stats):\n    meta_data = decentriq_util.read_tabular_data(\"/input/ViennaHF-Register-metadata\")\n    varname_col = [x for x in ['VARIABLE NAME', 'VARIABLENAME', 'VAR NAME'] if x in meta_data.columns][0]\n    metadata_vars = [x.lower().strip() for x in meta_data[varname_col].values]\n    meta_data.columns = [c.strip() + \" (metadata dictionary)\" if c.upper() != varname_col else c.strip() for c in meta_data.columns]\n    #print(\"vars from var_stats:\", vars_stats.keys())\n    #print(\"vars in metadata: \", metadata_vars)\n    #print(\" vars in common: \", [x for x in metadata_vars if x in vars_stats.keys()])\n    #print(\" vars no stats: \", [x for x in metadata_vars if x not in vars_stats.keys()])\n    additional_cols = set()\n    for s in vars_stats.values():\n        additional_cols.update(s.keys())\n    for c in additional_cols:\n        cvals = []\n        for vname in metadata_vars:\n            if not vname in vars_stats or not c in vars_stats[vname]:\n                cvals.append(None)\n            else:\n                cvals.append(vars_stats[vname][c])\n        meta_data[c] =cvals\n    meta_data.to_csv(\"/output/meta_data_enriched.csv\")\n    return meta_data\n\n\ndef dataframe_to_json_dicts(df):\n    varname_col = [x for x in ['VARIABLE NAME', 'VARIABLENAME', 'VAR NAME'] if x in df.columns][0]\n    json_dicts = {}\n    for _, row in df.iterrows():\n        variable_name = row[varname_col]\n        var_dict = {}\n        for col in df.columns:\n            if col not in [varname_col, 'Column'] and pd.notna(row[col]) and row[col] != \"\" :\n                try:\n                    valu = row[col].lower().strip()\n                except Exception:\n                    valu = row[col]\n                var_dict[col.lower()] = _convert_numeric(valu)\n        json_dicts[variable_name] = var_dict\n    with open(\"/output/eda_output_ViennaHF-Register.json\", 'w', encoding='utf-8') as f:\n        json.dump(json_dicts, f, indent=4)\n\n\ndef _convert_numeric(val):\n    try:\n        return int(val)\n    except (ValueError, TypeError):\n        try:\n            return float(val)\n        except (ValueError, TypeError):\n            return val\n\nvars_to_stats = variable_eda(data, vars_details)\nmeta_data_enriched = integrate_eda_with_metadata(vars_to_stats)\njson_dicts = dataframe_to_json_dicts(meta_data_enriched)\n\nwith open('/output/data_issues.json', 'w') as json_file:\n    json.dump(data_issues, json_file, indent=4)\n"}, "output": "/output", "scriptingLanguage": {"python": {}}, "scriptingSpecificationId": "decentriq.python-ml-worker-32-64", "staticContentSpecificationId": "decentriq.driver"}}}}}], "participants": [{"user": "anas.elghafari@maastrichtuniversity.nl", "permissions": [{"analyst": {"nodeId": "c1_data_dict_check"}}, {"analyst": {"nodeId": "c2_save_to_json"}}, {"analyst": {"nodeId": "c3_eda_data_profiling"}}, {"dataOwner": {"nodeId": "ViennaHF-Register"}}, {"dataOwner": {"nodeId": "ViennaHF-Register-metadata"}}, {"manager": {}}]}, {"user": "cohort-explorer-ids@maastrichtuniversity.nl", "permissions": [{"analyst": {"nodeId": "c1_data_dict_check"}}, {"analyst": {"nodeId": "c2_save_to_json"}}, {"analyst": {"nodeId": "c3_eda_data_profiling"}}, {"dataOwner": {"nodeId": "ViennaHF-Register-metadata"}}]}], "title": "iCARE4CVD DCR provision ViennaHF-Register"}}}}}